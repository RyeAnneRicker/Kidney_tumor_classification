{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to create the confusion matrix for the best classifier found from the modelling script\n",
    "# In each case, HGB was the best classifier, thus that is the only model shown here\n",
    "# The model is the same HGB model script from the script containing all the classifiers\n",
    "# Both Cyst vs pRCC and Tumor Grade/Type models are found below, make sure to run the correct one and only the correct one\n",
    "\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of     label      Mean  Variance   Skewness    Kurtosis    Energy   Entropy  \\\n",
       "7       2  1.021675  0.092855  14.176613  204.739909  0.989841  0.074144   \n",
       "15      3  1.105425  0.364261   5.678055   33.955817  0.940815  0.366153   \n",
       "2       2  1.148525  0.595365   5.123756   27.906581  0.928883  0.453395   \n",
       "8       2  1.005850  0.022916  25.957949  677.234672  0.997004  0.022784   \n",
       "12      3  1.103425  0.360378   5.813531   35.829910  0.941719  0.377513   \n",
       "14      3  1.027200  0.105660  11.930552  143.939337  0.986088  0.093635   \n",
       "19      3  1.081850  0.336751   7.027086   50.996743  0.961251  0.247869   \n",
       "16      3  1.041200  0.168203   9.923162  100.274341  0.980082  0.131322   \n",
       "10      2  1.063125  0.226590   7.559896   59.231871  0.965166  0.229476   \n",
       "18      3  1.123025  0.441540   5.329484   30.000273  0.934015  0.412132   \n",
       "21      3  1.055100  0.253064   9.126123   85.195224  0.976217  0.160908   \n",
       "3       2  1.062325  0.242191   8.107291   69.355176  0.967317  0.230134   \n",
       "11      2  1.032550  0.124740  10.941252  122.610635  0.982964  0.119983   \n",
       "17      2  1.010150  0.049297  21.963430  486.014837  0.995807  0.031382   \n",
       "0       3  1.071500  0.261238   7.455674   59.661344  0.959350  0.285630   \n",
       "4       3  1.097400  0.423113   6.615877   45.373744  0.956352  0.273468   \n",
       "1       3  1.085175  0.327870   6.669272   46.101109  0.956832  0.278971   \n",
       "6       2  1.040200  0.181184  10.623191  115.186505  0.982220  0.121540   \n",
       "5       2  1.098500  0.352398   6.017426   38.176524  0.946154  0.342862   \n",
       "20      2  1.111375  0.408121   5.658375   33.537328  0.941344  0.366488   \n",
       "9       2  1.035350  0.137250  10.747815  120.168507  0.981372  0.136069   \n",
       "13      3  1.089875  0.355897   6.615258   45.629964  0.955536  0.291848   \n",
       "\n",
       "         SRE           LRE          GLN  ...  Variance.1  Sum Average  \\\n",
       "7   0.000042  24431.895672   192.684329  ...  126.517130     3.589250   \n",
       "15  0.000063  16541.038851  1096.798719  ...  505.691052     9.743026   \n",
       "2   0.000048  22543.015069  1381.332929  ...  830.769907    12.884901   \n",
       "8   0.000053  19003.057420    51.354680  ...   29.778291     2.413295   \n",
       "12  0.000066  16724.668754  1131.177766  ...  502.263339     9.603683   \n",
       "14  0.000054  18882.464271   234.467818  ...  134.143704     3.921084   \n",
       "19  0.000044  23449.103861   716.025856  ...  466.270995     8.019370   \n",
       "16  0.000043  23623.272830   347.564099  ...  240.110941     5.086359   \n",
       "10  0.000060  17486.474501   648.558990  ...  311.973363     6.638229   \n",
       "18  0.000058  18122.732073  1236.201299  ...  619.028118    11.052279   \n",
       "21  0.000037  27772.642256   439.462814  ...  336.844344     5.992393   \n",
       "3   0.000060  20054.038995   637.185269  ...  334.796502     6.589766   \n",
       "11  0.000053  19682.850614   327.451087  ...  170.904191     4.391708   \n",
       "17  0.000034  30200.319709    73.604423  ...   64.841698     2.729613   \n",
       "0   0.000079  17202.928387   805.352223  ...  357.650914     7.233486   \n",
       "4   0.000039  26159.047344   785.791277  ...  585.991712     9.172958   \n",
       "1   0.000051  20506.649816   792.409436  ...  454.750456     8.261432   \n",
       "6   0.000038  26897.973287   327.142168  ...  243.440297     4.932255   \n",
       "5   0.000059  17964.692246   997.961420  ...  497.986521     9.307364   \n",
       "20  0.000056  18558.606752  1117.171912  ...  562.121091    10.137026   \n",
       "9   0.000059  19595.647272   362.362333  ...  185.890574     4.581177   \n",
       "13  0.000050  21420.570542   847.777502  ...  489.066392     8.575786   \n",
       "\n",
       "    Sum Variance  Sum Entropy  Difference Variance  Difference Entropy  \\\n",
       "7     485.264724    -0.686435            26.202781           -0.686555   \n",
       "15   1993.152400    -0.655701            42.819373           -0.656876   \n",
       "2    3259.547356    -0.648068            80.653644           -0.648903   \n",
       "8     113.159176    -0.691015             9.766880           -0.691105   \n",
       "12   1961.192044    -0.656294            60.898288           -0.656909   \n",
       "14    518.971768    -0.684049            23.435637           -0.684529   \n",
       "19   1825.683406    -0.668462            50.569187           -0.669213   \n",
       "16    931.613780    -0.680270            36.213259           -0.680847   \n",
       "10   1218.420880    -0.670938            38.830808           -0.671539   \n",
       "18   2434.890881    -0.651370            56.044785           -0.652626   \n",
       "21   1309.557900    -0.677876            46.403559           -0.678296   \n",
       "3    1293.945936    -0.672295            54.552991           -0.672550   \n",
       "11    659.326511    -0.682091            30.746855           -0.682249   \n",
       "17    243.711139    -0.690249            19.900492           -0.690362   \n",
       "0    1371.518357    -0.667362            69.185816           -0.667550   \n",
       "4    2299.865010    -0.665426            56.753038           -0.666484   \n",
       "1    1789.016008    -0.665676            41.428320           -0.666595   \n",
       "6     945.595788    -0.681635            35.354328           -0.681951   \n",
       "5    1955.854451    -0.659057            48.794726           -0.660135   \n",
       "20   2207.868498    -0.656002            54.346646           -0.656883   \n",
       "9     712.998752    -0.681097            37.268170           -0.681225   \n",
       "13   1910.854199    -0.664921            57.258448           -0.665520   \n",
       "\n",
       "    Information Measure of Correlation 1  \\\n",
       "7                               0.003353   \n",
       "15                              0.021387   \n",
       "2                               0.025908   \n",
       "8                               0.000890   \n",
       "12                              0.021020   \n",
       "14                              0.004671   \n",
       "19                              0.013711   \n",
       "16                              0.006819   \n",
       "10                              0.012227   \n",
       "18                              0.023974   \n",
       "21                              0.008207   \n",
       "3                               0.011412   \n",
       "11                              0.005776   \n",
       "17                              0.001300   \n",
       "0                               0.014358   \n",
       "4                               0.015513   \n",
       "1                               0.015337   \n",
       "6                               0.006046   \n",
       "5                               0.019341   \n",
       "20                              0.021206   \n",
       "9                               0.006339   \n",
       "13                              0.015828   \n",
       "\n",
       "    Information Measure of Correlation 2  \\\n",
       "7                               0.104375   \n",
       "15                              0.252006   \n",
       "2                               0.274384   \n",
       "8                               0.054135   \n",
       "12                              0.250107   \n",
       "14                              0.122766   \n",
       "19                              0.205616   \n",
       "16                              0.147522   \n",
       "10                              0.194898   \n",
       "18                              0.265166   \n",
       "21                              0.161290   \n",
       "3                               0.188696   \n",
       "11                              0.136153   \n",
       "17                              0.065330   \n",
       "0                               0.210129   \n",
       "4                               0.217728   \n",
       "1                               0.216600   \n",
       "6                               0.139194   \n",
       "5                               0.240873   \n",
       "20                              0.251063   \n",
       "9                               0.142431   \n",
       "13                              0.219794   \n",
       "\n",
       "    Inverse Difference moment normalized  Invere Difference Normalized  \n",
       "7                               0.999429                      0.999710  \n",
       "15                              0.997936                      0.999452  \n",
       "2                               0.996793                      0.999033  \n",
       "8                               0.999783                      0.999883  \n",
       "12                              0.997257                      0.999232  \n",
       "14                              0.999439                      0.999724  \n",
       "19                              0.998548                      0.999424  \n",
       "16                              0.999170                      0.999597  \n",
       "10                              0.998607                      0.999529  \n",
       "18                              0.997724                      0.999317  \n",
       "21                              0.998875                      0.999495  \n",
       "3                               0.997891                      0.999312  \n",
       "11                              0.999139                      0.999642  \n",
       "17                              0.999683                      0.999794  \n",
       "0                               0.997253                      0.999115  \n",
       "4                               0.998476                      0.999370  \n",
       "1                               0.998569                      0.999502  \n",
       "6                               0.999197                      0.999610  \n",
       "5                               0.998213                      0.999417  \n",
       "20                              0.997850                      0.999341  \n",
       "9                               0.998781                      0.999549  \n",
       "13                              0.997947                      0.999325  \n",
       "\n",
       "[22 rows x 38 columns]>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the data\n",
    "df = pd.read_excel('/Users/RyeAnne/Documents/LoewLab/KidneyCancer/FeatureMatrices/FM_CM_Grade.xls') # import .xls file\n",
    "df = df.sample(frac = 1) \n",
    "df.shape\n",
    "df.head # view the first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into predictor and response variables\n",
    "y = df['label'].ravel() # set the dependent variable, which is the column labelled 'label'\n",
    "x = df.drop(['label'], axis = 1) # set the predictor variables, which is all columns but 'label'\n",
    "le = preprocessing.LabelEncoder() # turns result into binary \"0\" or \"1\" result\n",
    "y = le.fit_transform(y) # applies le to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the level of features you want to use\n",
    "selection = SelectPercentile(f_classif, percentile=80).fit(x, y) # makes sure to set the percentile of features you want to use\n",
    "x = selection.transform(x) # edits predictor variables to only use top x-percentile of features that you specified in the previous line\n",
    "scaler = StandardScaler().fit(x) # normalizes data\n",
    "x = scaler.transform(x) # applied normalization\n",
    "scoring ={ \"Accuracy\":'accuracy',\"AUC\":'roc_auc'} # specifies to use AUC as the method of % accuracy, as the data is imbalanced- only use this method to create the model\n",
    "s = StratifiedShuffleSplit(n_splits=3, random_state=10) # n_splits=3 for Grade/Type, n_splits=10 for Cyst vs pRCC\n",
    "\n",
    "#selection = SelectPercentile(f_classif, percentile=20).fit(x, y)\n",
    "#x = selection.transform(x)\n",
    "#sum(np.array(selection.pvalues_ < 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train - not used since data set is too small to separate into testing and training, must use whole set for test-okay since previous script used test/train sets and AUC method\n",
    "#xt, xv, yt, yv = train_test_split(x, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB: 0.5 {'learning_rate': 0.001, 'max_iter': 10}\n",
      "HBG: 1.0 {'max_depth': None, 'max_leaf_nodes': 16, 'min_samples_leaf': 4}\n"
     ]
    }
   ],
   "source": [
    "# ***Use for Cyst Runs***\n",
    "\n",
    "\n",
    "# build model-hgb\n",
    "\n",
    "\n",
    "params = {'learning_rate': [0.001, 0.003, 0.01, 0.03, 0.05, 0.1, 0.2, 0.3], # set first set of parameters\n",
    "         'max_iter': range(10, 301, 20)}\n",
    "hgb_search2 = GridSearchCV(HistGradientBoostingClassifier(n_iter_no_change=50),  # first model using variation is first set of parameters\n",
    "                                   params,cv=s.split(x, y), scoring=scoring, refit='AUC')\n",
    "hgb_search2.fit(x, y) # fit first model\n",
    "print('HGB:', hgb_search2.best_score_, hgb_search2.best_params_) # print results of first model\n",
    "\n",
    "#params = {'l2_regularization': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]} -found not to work well\n",
    "#hgb_search3 = GridSearchCV(hgb_search2.best_estimator_, params,cv=s.split(x, y), scoring=scoring, refit='AUC')\n",
    "#hgb_search3.fit(x, y)\n",
    "#print('HGB:',hgb_search3.best_score_, hgb_search3.best_params_)\n",
    "\n",
    "params = {'max_depth': [None, 4,5, 6, 7, 8, 9, 10], # second set of parameters to vary\n",
    "                  'min_samples_leaf': range(2, 41, 2),\n",
    "                  'max_leaf_nodes': range(2, 41, 2)\n",
    "                 }\n",
    "hgb_search1 = GridSearchCV(hgb_search2.best_estimator_, # build model\n",
    "                                   params, cv=s.split(x, y), scoring=scoring, refit='AUC')\n",
    "HGB = hgb_search1.fit(x, y) # set final model\n",
    "cvres = hgb_search1.cv_results_ # results\n",
    "print('HBG:', hgb_search1.best_score_, hgb_search1.best_params_) # display final accuracy results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB: 1.0 {'learning_rate': 0.001, 'max_depth': None, 'max_iter': 60, 'max_leaf_nodes': 32, 'min_samples_leaf': 3}\n"
     ]
    }
   ],
   "source": [
    "# ***Use for Grade/Type Runs***\n",
    "\n",
    "# Build Model\n",
    "\n",
    "params = {'learning_rate': [0.001, 0.003, 0.01, 0.03], # set parameters to vary-use exhaustive search\n",
    "                 'max_iter': [40,60,80,100],\n",
    "                 'max_depth': [None, 2,4,6],\n",
    "                  'min_samples_leaf': [1, 2, 3],\n",
    "                  'max_leaf_nodes': range(2, 41, 5)\n",
    "                 }\n",
    "hgb_search2 = GridSearchCV(HistGradientBoostingClassifier(n_iter_no_change=50), # model\n",
    "                                   params,cv=s.split(x, y), scoring=scoring, refit='AUC')\n",
    "HGB = hgb_search2.fit(x, y) # fit the model to the data\n",
    "print('HGB:', hgb_search2.best_score_, hgb_search2.best_params_) # print the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on the validation set\n",
    "y_pred_hgb = HGB.predict(x) # using the model, predict the results of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y) # display actual results\n",
    "print('')\n",
    "print(y_pred_hgb) # display predicted results\n",
    "\n",
    "# Note-if you have the algorithm classify all data as one type (all 0 or all 1), redo entire method as this was due to an imbalance in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make the confusion matrix\n",
    "cm_hgb = confusion_matrix(y, (y_pred_hgb>0.5)) # make confusion matrix using truth and predicted results\n",
    "print('The Confusion Matrix is: ','\\n', cm_hgb) # print the confusion matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy on test set\n",
    "predict_accuracy_on_HGB_test_set = (cm_hgb[0,0] + cm_hgb[1,1])/(cm_hgb[0,0] + cm_hgb[1,1]+cm_hgb[1,0] + cm_hgb[0,1]) # calculate accuracy using the numbers in the positions of the matrix\n",
    "print('The Accuracy on Test Set is: ', predict_accuracy_on_HGB_test_set) # print the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
